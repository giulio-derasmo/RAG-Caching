{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45bf7d1-c1a0-4299-b158-0ed8759205ba",
   "metadata": {},
   "source": [
    "## Load dataset SQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356f04d4-02b5-45f5-bd21-7c37e08e3f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\giuli\\\\Documents\\\\ragcache\\\\code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f983bfbc-2f35-4e82-82a8-cd941281d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train_ds, val_ds = load_dataset(\"squad\", split=['train', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de61444c-e750-424a-8f0a-1db280cd2bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "     num_rows: 87599\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "     num_rows: 10570\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f9af7-85c6-4c5b-8b31-ed6a6e1edb92",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a53a0a7f-04a0-4828-b498-62f9d24422ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings # embedder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c16d57-0222-48b7-a143-743985dca39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def faiss_knn(xb, xq, k = 3, d = 1024):\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(xb) \n",
    "    D, I = index.search(xq, k) \n",
    "    return I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa10e7c-7ae2-4022-9578-f942810d2f1e",
   "metadata": {},
   "source": [
    "#1. embed questions \n",
    "\n",
    "#2. compute knn (k=3) to extract pairs and lebaled them \n",
    "\n",
    "#3. build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46e4fa7a-15b2-4855-b6c6-eac1cb0bb654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(dataset_split, embedding_model, k = 6, d = 1024):\n",
    "    '''\n",
    "    input: dataset, embedding_model\n",
    "    output: dataset pairs D = (q1,q0,y)\n",
    "    '''\n",
    "    \n",
    "    # i have X questions per context -> I want to create a context_id to every question with same context\n",
    "    df = dataset_split.to_pandas()\n",
    "    df['p_id'] = df['context'].factorize()[0]\n",
    "    df.drop_duplicates(subset='question', keep=\"first\", inplace=True)\n",
    "    # create an identifier \n",
    "    df['matrix_id'] = list(range(0, len(df)))   \n",
    "\n",
    "    print('stage faiss knn')\n",
    "    # embeddings\n",
    "    q_embeddings = embedding_model.embed_documents(df.question.to_list())\n",
    "    # setup faiss knn\n",
    "    xb = np.array(q_embeddings, dtype = np.float32)\n",
    "    I = faiss_knn(xb = xb, xq = xb, k = k, d = d)\n",
    "\n",
    "    #We sample 1999 prompts uniformly at random, and for each prompt, we choose the farthest three prompts 3\n",
    "    #among the five nearest neighbors to form three prompt pairs. \n",
    "    indexes = np.random.choice(range(0,I.shape[0]), size=1999, replace=False)\n",
    "    I = I[indexes, 2:]\n",
    "    # create dataset pairs and label whether each prompt pair can be answered by the same corpus (0 or 1)\n",
    "    print('stage dataset creation')\n",
    "    dataset = []\n",
    "    for row in tqdm(I):\n",
    "        # (center, neighb1, neighb2, neighb3)\n",
    "        center = row[0]\n",
    "        passage_id0 = df.loc[df.matrix_id == center, 'p_id'].values[0]\n",
    "        question0 = df.loc[df.matrix_id == center, 'question'].values[0]\n",
    "        for neigh in row[1:]:\n",
    "            passage_id1 = df.loc[df.matrix_id == neigh, 'p_id'].values[0]\n",
    "            question1 = df.loc[df.matrix_id == neigh, 'question'].values[0]\n",
    "            # append (q0, q1, y)\n",
    "            if passage_id0 == passage_id1:\n",
    "                dataset.append( (question0, question1, 1) )\n",
    "            else:\n",
    "                dataset.append( (question0, question1, 0) )\n",
    "                \n",
    "    return dataset        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6a9cc-0ea7-4032-9a3c-4844d196dc11",
   "metadata": {},
   "source": [
    "### KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82dc90b2-203e-4770-9d4b-2ad97549e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = 'intfloat/e5-large-v2'\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # set True for cosine similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcbba59b-6764-4a16-a257-460e3121c7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage faiss knn\n",
      "stage dataset creation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964d3994f6c14d52b25eb0abbb13d123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = create_pairs(train_ds, embedding_model, k = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07cf31f8-a4e1-4104-872c-eedfd6c3b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r'C:/Users/giuli/Documents/ragcache/data/train_squadv0.pickle', 'wb') as handle:\n",
    "    pickle.dump(train, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8f94db1-bea8-4f47-bee0-21a1f9d883ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 5294, 1: 703})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([sample[2] for sample in train])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c9f11-a3cb-4e3e-8e79-1e3c6de2a1aa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a11712ff-b4aa-4526-970d-ac3d299a93fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import LoggingHandler, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2c1bd0f-11f5-4d1a-8840-fba33b3bd0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare data\n",
    "train_df = pd.DataFrame(train)\n",
    "train_s, test_s = train_test_split(train_df, test_size=0.3, random_state=0, stratify=train_df.loc[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f74ad2e9-89be-45cd-aff7-c2e8f4a92c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_s = [train[idx] for idx in train_s.index]\n",
    "val_s = [train[idx] for idx in test_s.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7a4a5a3f-8326-4996-bf62-a79477d27103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 3705, 1: 492}), Counter({0: 1589, 1: 211}))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([sample[2] for sample in train_s]), Counter([sample[2] for sample in val_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1f4f64eb-4bab-44a5-a2f7-b1f54214f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = []\n",
    "\n",
    "for sample in train_s:\n",
    "    train_samples.append(InputExample(texts=[sample[0][0], sample[1][0]], label=sample[2]))\n",
    "\n",
    "dev_samples = []\n",
    "for sample in val_s:\n",
    "    dev_samples.append(InputExample(texts=[sample[0][0], sample[1][0]], label=sample[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3a7b1720-6edd-4213-9b1e-ae9b3840bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 32\n",
    "num_epochs = 5\n",
    "model_save_path = r'C:/Users/giuli/Documents/ragcache/models/' + model_name + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "model_name = \"distilroberta-base\"\n",
    "model = CrossEncoder(model_name, num_labels=1)\n",
    "\n",
    "# We wrap train_samples (which is a List[InputExample]) into a pytorch DataLoader\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0942eea1-de86-4814-ae58-109db4c29761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add an evaluator, which evaluates the performance during training\n",
    "evaluator = CEBinaryClassificationEvaluator.from_input_examples(dev_samples, name=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ac12864-d642-4606-a79e-43308832d1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the training\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)  # 10% of train data for warm-up\n",
    "warmup_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c74674bb-7dfd-4698-a5ee-8468e88688bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def sigmoid(x):\n",
    "    return torch.round(torch.sigmoid(x/(0.01-80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cb99c4c5-1c21-4367-9d9a-31d869bae0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0ae4d1ecfb4c669c115fe48bf322f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d605e1216544ccaa8ed5e63f3ac54b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4234a622bbd04de58723460576b8b90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0aea1a49ab451ba039675261e98ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cf4cb5d2234865bb6c7fdcf50069c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbf7d761edd40b9a4e43b678d8932d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=model_save_path,\n",
    "    #activation_fct = sigmoid,\n",
    "    #loss_fct = torch.nn.CrossEntropyLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "77e111f6-89d3-456d-bfa1-c246cb7687de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c943ecefa0534013878303c77ac3cfc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/93.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if dataset exists. If not, download and extract  it\n",
    "dataset_path = \"quora-dataset/\"\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    zip_save_path = \"quora-IR-dataset.zip\"\n",
    "    util.http_get(url=\"https://sbert.net/datasets/quora-IR-dataset.zip\", path=zip_save_path)\n",
    "    with ZipFile(zip_save_path, \"r\") as zip:\n",
    "        zip.extractall(dataset_path)\n",
    "\n",
    "\n",
    "# Read the quora dataset split for classification\n",
    "train_samples = []\n",
    "with open(os.path.join(dataset_path, \"classification\", \"train_pairs.tsv\"), \"r\", encoding=\"utf8\") as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        train_samples.append(InputExample(texts=[row[\"question1\"], row[\"question2\"]], label=int(row[\"is_duplicate\"])))\n",
    "        train_samples.append(InputExample(texts=[row[\"question2\"], row[\"question1\"]], label=int(row[\"is_duplicate\"])))\n",
    "\n",
    "\n",
    "dev_samples = []\n",
    "with open(os.path.join(dataset_path, \"classification\", \"dev_pairs.tsv\"), \"r\", encoding=\"utf8\") as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        dev_samples.append(InputExample(texts=[row[\"question1\"], row[\"question2\"]], label=int(row[\"is_duplicate\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6848d5e0-58c3-4807-8a8b-458333606e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 349308, 1: 207326})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([doc.label for doc in train_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2ad8bcd1-c873-4851-87a4-8ce53d324dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 24012, 1: 12959})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([doc.label for doc in dev_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c50422a6-3824-4a35-b6f5-df7bf2e75cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9a1d6ba4c049b4bd60f1b3013a4731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3327edc9222c4296a7991246d8555dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/8698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd74dc9f54a45f7bfe90ea4c5a4fc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/8698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db85c03b289045609010aa514c047c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/8698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e04cf71ca7454b8ba05f554b2d7270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/8698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration\n",
    "train_batch_size = 16\n",
    "num_epochs = 4\n",
    "model_save_path =  r'C:/Users/giuli/Documents/ragcache/models/training_quora-' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "# We use distilroberta-base with a single label, i.e., it will output a value between 0 and 1 indicating the similarity of the two questions\n",
    "model = CrossEncoder(\"distilroberta-base\", num_labels=1)\n",
    "\n",
    "# We wrap train_samples (which is a List[InputExample]) into a pytorch DataLoader\n",
    "train_dataloader = DataLoader(train_samples[:int(len(train_samples)/4)], shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "\n",
    "# We add an evaluator, which evaluates the performance during training\n",
    "evaluator = CEBinaryClassificationEvaluator.from_input_examples(dev_samples[:int(len(dev_samples)/4)], name=\"Quora-dev\")\n",
    "\n",
    "\n",
    "# Configure the training\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)  # 10% of train data for warm-up\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=5000,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=model_save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b08cdf-7e6a-49a5-aa85-967e5a621806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
